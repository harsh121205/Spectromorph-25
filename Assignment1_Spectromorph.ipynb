{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WfIfaHIWlGq",
        "outputId": "4f27d52e-2355-4cbc-c869-44533e2cec32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 378 images belonging to 18 classes.\n",
            "Found 162 images belonging to 18 classes.\n",
            "Epoch 1/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 430ms/step - accuracy: 0.0450 - loss: 4.9065 - val_accuracy: 0.0741 - val_loss: 3.7492\n",
            "Epoch 2/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - accuracy: 0.0603 - loss: 3.5420 - val_accuracy: 0.0494 - val_loss: 3.0886\n",
            "Epoch 3/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - accuracy: 0.0738 - loss: 3.0296 - val_accuracy: 0.0988 - val_loss: 2.8826\n",
            "Epoch 4/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 213ms/step - accuracy: 0.0974 - loss: 2.8436 - val_accuracy: 0.1543 - val_loss: 2.6354\n",
            "Epoch 5/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.1708 - loss: 2.6186 - val_accuracy: 0.2654 - val_loss: 2.4592\n",
            "Epoch 6/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 259ms/step - accuracy: 0.2233 - loss: 2.4812 - val_accuracy: 0.2469 - val_loss: 2.3447\n",
            "Epoch 7/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 281ms/step - accuracy: 0.2923 - loss: 2.2901 - val_accuracy: 0.3642 - val_loss: 2.1127\n",
            "Epoch 8/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 216ms/step - accuracy: 0.2968 - loss: 2.2146 - val_accuracy: 0.3580 - val_loss: 2.2007\n",
            "Epoch 9/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 211ms/step - accuracy: 0.3031 - loss: 2.3031 - val_accuracy: 0.4074 - val_loss: 2.0470\n",
            "Epoch 10/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 210ms/step - accuracy: 0.4224 - loss: 2.0690 - val_accuracy: 0.4074 - val_loss: 1.9816\n",
            "Epoch 11/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.3339 - loss: 2.1711 - val_accuracy: 0.3951 - val_loss: 2.0740\n",
            "Epoch 12/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.4107 - loss: 2.1471 - val_accuracy: 0.4259 - val_loss: 1.8881\n",
            "Epoch 13/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.4814 - loss: 1.9318 - val_accuracy: 0.4506 - val_loss: 1.9073\n",
            "Epoch 14/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - accuracy: 0.4048 - loss: 1.9404 - val_accuracy: 0.4012 - val_loss: 1.9446\n",
            "Epoch 15/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 296ms/step - accuracy: 0.4309 - loss: 1.9436 - val_accuracy: 0.4815 - val_loss: 1.9171\n",
            "Epoch 16/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.5133 - loss: 1.8933 - val_accuracy: 0.4259 - val_loss: 1.8658\n",
            "Epoch 17/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.4875 - loss: 1.8509 - val_accuracy: 0.5185 - val_loss: 1.7220\n",
            "Epoch 18/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 209ms/step - accuracy: 0.5005 - loss: 1.8071 - val_accuracy: 0.5123 - val_loss: 1.7378\n",
            "Epoch 19/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 276ms/step - accuracy: 0.5341 - loss: 1.5926 - val_accuracy: 0.5062 - val_loss: 1.6665\n",
            "Epoch 20/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 234ms/step - accuracy: 0.4668 - loss: 1.9329 - val_accuracy: 0.4938 - val_loss: 1.7824\n",
            "Epoch 21/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.5803 - loss: 1.5608 - val_accuracy: 0.5370 - val_loss: 1.7338\n",
            "Epoch 22/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - accuracy: 0.5665 - loss: 1.6154 - val_accuracy: 0.5741 - val_loss: 1.6327\n",
            "Epoch 23/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - accuracy: 0.5887 - loss: 1.5722 - val_accuracy: 0.5679 - val_loss: 1.6103\n",
            "Epoch 24/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 268ms/step - accuracy: 0.5914 - loss: 1.5679 - val_accuracy: 0.5679 - val_loss: 1.6260\n",
            "Epoch 25/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 216ms/step - accuracy: 0.5513 - loss: 1.5988 - val_accuracy: 0.5494 - val_loss: 1.6412\n",
            "Epoch 26/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 213ms/step - accuracy: 0.5892 - loss: 1.5485 - val_accuracy: 0.5556 - val_loss: 1.7010\n",
            "Epoch 27/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - accuracy: 0.6467 - loss: 1.5006 - val_accuracy: 0.5370 - val_loss: 1.5852\n",
            "Epoch 28/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 286ms/step - accuracy: 0.5554 - loss: 1.5780 - val_accuracy: 0.5185 - val_loss: 1.7740\n",
            "Epoch 29/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - accuracy: 0.5851 - loss: 1.5334 - val_accuracy: 0.6605 - val_loss: 1.6172\n",
            "Epoch 30/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.6096 - loss: 1.4978 - val_accuracy: 0.6173 - val_loss: 1.4937\n",
            "Epoch 31/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.5928 - loss: 1.4778 - val_accuracy: 0.6420 - val_loss: 1.4455\n",
            "Epoch 32/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - accuracy: 0.6804 - loss: 1.4438 - val_accuracy: 0.5741 - val_loss: 1.5267\n",
            "Epoch 33/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 297ms/step - accuracy: 0.6165 - loss: 1.3904 - val_accuracy: 0.6358 - val_loss: 1.4733\n",
            "Epoch 34/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - accuracy: 0.6438 - loss: 1.4104 - val_accuracy: 0.6235 - val_loss: 1.5223\n",
            "Epoch 35/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 219ms/step - accuracy: 0.6499 - loss: 1.4040 - val_accuracy: 0.6296 - val_loss: 1.5565\n",
            "Epoch 36/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 229ms/step - accuracy: 0.6185 - loss: 1.4724 - val_accuracy: 0.6296 - val_loss: 1.4848\n",
            "Epoch 37/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - accuracy: 0.6704 - loss: 1.3409 - val_accuracy: 0.6420 - val_loss: 1.4844\n",
            "Epoch 38/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 272ms/step - accuracy: 0.6588 - loss: 1.3365 - val_accuracy: 0.6420 - val_loss: 1.5023\n",
            "Epoch 39/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - accuracy: 0.6836 - loss: 1.3629 - val_accuracy: 0.6235 - val_loss: 1.5199\n",
            "Epoch 40/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 219ms/step - accuracy: 0.6598 - loss: 1.3826 - val_accuracy: 0.5802 - val_loss: 1.4994\n",
            "Epoch 41/50\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - accuracy: 0.6554 - loss: 1.4319 - val_accuracy: 0.6049 - val_loss: 1.5113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/dataset'\n",
        "IMAGE_SIZE = (64, 64)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.10,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_gen = train_datagen.flow_from_directory(\n",
        "    DATA_DIR,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Model definition\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(train_gen.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "early_stop = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "\n",
        "model.save('emoji_cnn_model.h5')\n",
        "\n",
        "\n",
        "def predict_emoji(image_path, model, class_indices):\n",
        "    img = load_img(image_path, target_size=IMAGE_SIZE, color_mode='grayscale')\n",
        "    arr = img_to_array(img) / 255.0\n",
        "    arr = np.expand_dims(arr, axis=0)\n",
        "    preds = model.predict(arr)\n",
        "    idx = np.argmax(preds, axis=1)[0]\n",
        "    inv_map = {v: k for k, v in class_indices.items()}\n",
        "    return inv_map[idx], preds[0][idx]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from tensorflow.keras.models import load_model\n",
        "    clf = load_model('emoji_cnn_model.h5')\n",
        "    class_map = train_gen.class_indices\n",
        "\n",
        ""
      ]
    }
  ]
}